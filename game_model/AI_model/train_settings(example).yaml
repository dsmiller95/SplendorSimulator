learning_rate: 0.001 
gamma: 0.99 #discount factor, how much it cares about future reward vs current reward
            #(0: only current, 1: current and all future states)
epsilon: 0.5 #how often to pick the maximum-Q-valued action
             # 0: always pick the maximum. 1: always pick random
memory_length: 10000      #number of rounds to play of the game
reps_per_play_sess: 10 #how many times to train over the same replay memory buffer
batch_size: 1000 #so that we don't run out of memory accidentally
epochs: 100000 #how many play->learn cycles to run

# Model definition (WARNING! If you change this, it will be incompatible with/overwrite the previous saved model)
hidden_layer_width: 128 #I like to keep things like linear layer widths at multiples of 2 for faster GPU processing
n_hidden_layers: 3

# Rewards: [use this reward?, value of this reward]
tokens_held: [False,1.0]
cards_held: [False,1.0]
points: [True,5.0]
win_lose: [True,200]
length_of_game: [True,-0.1]

force_cpu_turns: False
force_cpu_learning: False

