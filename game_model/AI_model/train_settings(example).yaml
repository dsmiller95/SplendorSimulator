# Train settings
learning_rate: 0.02 
gamma: 0.9 #discount factor, how much it cares about future reward vs current reward
           #(0: only current, 1: current and all future states)
epsilon: 0.1 #how often to pick a random action
batch_size: 1000
reps_per_play_sess: 1 #how many times to train over the same replay memory buffer
epochs: 100000 #how many play->learn cycles to run

# Model definition (WARNING! If you change this, it will over-write the previous saved model with one that won't load if you revert the changes)
hidden_layer_width: 1032 #I like to keep things like linear layer widths at multiples of 2 for faster GPU processing
n_hidden_layers: 64

# Rewards: [use this reward?, value of this reward]
tokens_held: [False,1.0]
cards_held: [False,0.5]
points: [True,5.0]
win_lose: [True,50]
length_of_game: [False,-0.5]
joker_coeff: [True, 5.0]

# Play settings
memory_length: 1000      #number of rounds to play of the game
randomize_player_num: True

# Other settings
play_device: "cuda"
learn_device: "cuda"
load_saved: True